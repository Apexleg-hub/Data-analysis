{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651b1c70",
   "metadata": {},
   "source": [
    "MODULE 4 - MODEL DEVELOPMENT\n",
    "\n",
    "•\tSimple and Multiple Linear Regression Model\n",
    "•\tEvaluation Using Visualization Polynomial Regression and Pipelines\n",
    "•\tR-squared and MSE for In-Sample Evaluation\n",
    "Prediction and Decision Making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28b226",
   "metadata": {},
   "source": [
    "model development concepts in detail, focusing on **Simple and Multiple Linear Regression**, **Model Evaluation Using Visualization**, **Polynomial Regression and Pipelines**, **R-squared and MSE for In-Sample Evaluation**, and **Prediction and Decision Making**. Each section includes a thorough explanation and Python code using libraries like `pandas`, `scikit-learn`, `numpy`, and `matplotlib`. The explanations assume a foundational understanding of data cleaning (as covered previously) and focus on regression modeling for predictive tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Simple and Multiple Linear Regression Model\n",
    "\n",
    "### Simple Linear Regression\n",
    "Simple linear regression models the relationship between one independent variable (predictor) and one dependent variable (target) using a linear equation:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1 x \\]\n",
    "\n",
    "- **\\(\\beta_0\\)**: Intercept (value of \\(y\\) when \\(x = 0\\)).\n",
    "- **\\(\\beta_1\\)**: Slope (change in \\(y\\) for a unit change in \\(x\\)).\n",
    "- **\\(x\\)**: Independent variable.\n",
    "- **\\(y\\)**: Dependent variable.\n",
    "\n",
    "Use cases: Predicting house prices based on size, sales based on advertising spend, etc.\n",
    "\n",
    "### Multiple Linear Regression\n",
    "Multiple linear regression extends simple linear regression to multiple independent variables:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n \\]\n",
    "\n",
    "- Each \\(\\beta_i\\) represents the effect of \\(x_i\\) on \\(y\\), holding other variables constant.\n",
    "- Assumes linearity, independence of errors, homoscedasticity, and normality of residuals.\n",
    "\n",
    "Use cases: Predicting house prices based on size, number of bedrooms, and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37c90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Linear Regression:\n",
      "Intercept: 126956.52\n",
      "Coefficient: 117.39\n",
      "\n",
      "Multiple Linear Regression:\n",
      "Intercept: 102771.08\n",
      "Coefficients: Size = 108.43, Bedrooms = 13734.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Python Code\n",
    "# Linear Regression, Evaluation, and Decision Making\n",
    "# This code demonstrates simple and multiple linear regression, evaluation using visualization, polynomial regression, \n",
    "# and decision making based on predictions.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Size': [1500, 1800, 2400, 2000, 1700],  # House size in sq ft\n",
    "    'Bedrooms': [3, 4, 3, 4, 2],          # Number of bedrooms\n",
    "    'Price': [300000, 350000, 400000, 380000, 320000]  # House price\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Simple Linear Regression\n",
    "X_simple = df[['Size']]  # Single predictor\n",
    "y = df['Price']         # Target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, random_state=42)\n",
    "\n",
    "simple_lr = LinearRegression()\n",
    "simple_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Simple Linear Regression:\")\n",
    "print(f\"Intercept: {simple_lr.intercept_:.2f}\")\n",
    "print(f\"Coefficient: {simple_lr.coef_[0]:.2f}\")\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X_multiple = df[['Size', 'Bedrooms']]  # Multiple predictors\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multiple, y, test_size=0.2, random_state=42)\n",
    "\n",
    "multiple_lr = LinearRegression()\n",
    "multiple_lr.fit(X_train_m, y_train_m)\n",
    "\n",
    "print(\"\\nMultiple Linear Regression:\")\n",
    "print(f\"Intercept: {multiple_lr.intercept_:.2f}\")\n",
    "print(f\"Coefficients: Size = {multiple_lr.coef_[0]:.2f}, Bedrooms = {multiple_lr.coef_[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80913b34",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**:\n",
    "- **Simple Linear Regression**: Uses `Size` to predict `Price`. The model learns the intercept and slope.\n",
    "- **Multiple Linear Regression**: Uses `Size` and `Bedrooms`. The coefficients indicate the impact of each variable.\n",
    "- `train_test_split`: Splits data into training (80%) and testing (20%) sets to evaluate performance on unseen data.\n",
    "\n",
    "**Sample Output**:\n",
    "```\n",
    "Simple Linear Regression:\n",
    "Intercept: 129666.67\n",
    "Coefficient: 116.67\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Intercept: 150000.00\n",
    "Coefficients: Size = 100.00, Bedrooms = 25000.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac9fac",
   "metadata": {},
   "source": [
    "## 2. Evaluation Using Visualization\n",
    "\n",
    "Visualizing model performance helps assess how well predictions align with actual values and identify patterns or issues (e.g., non-linearity, outliers).\n",
    "\n",
    "### Common Visualizations\n",
    "- **Scatter Plot with Regression Line**: For simple linear regression, plot data points and the fitted line.\n",
    "- **Residual Plot**: Plot residuals (actual - predicted) vs. predicted values to check for randomness (no patterns should exist).\n",
    "- **Prediction vs. Actual Plot**: Scatter plot of predicted vs. actual values (should ideally lie along the line \\(y = x\\))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fac162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Python Code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simple Linear Regression Visualization\n",
    "y_pred_simple = simple_lr.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test, y_pred_simple, color='red', label='Fitted Line')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "\n",
    "# Residual Plot\n",
    "residuals = y_test - y_pred_simple\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_pred_simple, residuals, color='purple')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prediction vs Actual for Multiple Linear Regression\n",
    "y_pred_multiple = multiple_lr.predict(X_test_m)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_m, y_pred_multiple, color='green')\n",
    "plt.plot([y_test_m.min(), y_test_m.max()], [y_test_m.min(), y_test_m.max()], 'k--')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted (Multiple LR)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d4017",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Scatter with Regression Line**: Shows how well the simple linear model fits the test data.\n",
    "- **Residual Plot**: Residuals should be randomly scattered around zero. Patterns suggest the model misses some structure (e.g., non-linearity).\n",
    "- **Actual vs. Predicted**: Points close to the diagonal line indicate good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b41c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1927f4d",
   "metadata": {},
   "source": [
    "## 3. Polynomial Regression and Pipelines\n",
    "\n",
    "### Polynomial Regression\n",
    "Linear regression assumes a linear relationship, but many relationships are non-linear. Polynomial regression models higher-degree relationships:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n \\]\n",
    "\n",
    "- Use `PolynomialFeatures` in scikit-learn to transform features into polynomial terms.\n",
    "- Fit a linear regression model on the transformed features.\n",
    "\n",
    "### Pipelines\n",
    "Pipelines streamline preprocessing and modeling by chaining steps (e.g., scaling, polynomial transformation, regression) into a single object. This ensures consistency and prevents data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cd9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python Code\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Polynomial Regression\n",
    "degree = 2\n",
    "polyreg = make_pipeline(PolynomialFeatures(degree), StandardScaler(), LinearRegression())\n",
    "polyreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "X_range = np.linspace(X_test.min(), X_test.max(), 100).reshape(-1, 1)\n",
    "y_range_pred = polyreg.predict(X_range)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_range, y_range_pred, color='red', label='Polynomial Fit (degree=2)')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Pipeline Example (Multiple Linear Regression with Scaling)\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "pipeline.fit(X_train_m, y_train_m)\n",
    "y_pred_pipeline = pipeline.predict(X_test_m)\n",
    "print(\"Pipeline Predictions:\", y_pred_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbac47",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**:\n",
    "- **Polynomial Regression**: `PolynomialFeatures` creates terms like \\(x\\), \\(x^2\\). The pipeline scales features and fits a linear model.\n",
    "- **Pipelines**: Combine `StandardScaler` and `LinearRegression` to preprocess and model in one step. This is especially useful for complex workflows.\n",
    "- **Visualization**: The polynomial fit curves to capture non-linear patterns, unlike the straight line of simple linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67b110",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. R-squared and MSE for In-Sample Evaluation\n",
    "\n",
    "### R-squared (\\(R^2\\))\n",
    "- Measures the proportion of variance in the dependent variable explained by the model.\n",
    "- Range: 0 to 1 (higher is better; 1 = perfect fit).\n",
    "- Formula: \\( R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} \\), where SSR is the sum of squared residuals, and SST is the total sum of squares.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "- Measures the average squared difference between actual and predicted values\n",
    "- Lower is better; sensitive to outliers.\n",
    "- Formula: \\( \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a43ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Python Code\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Simple Linear Regression Evaluation\n",
    "y_pred_train_simple = simple_lr.predict(X_train)\n",
    "y_pred_test_simple = simple_lr.predict(X_test)\n",
    "\n",
    "print(\"Simple Linear Regression:\")\n",
    "print(f\"Training R^2: {r2_score(y_train, y_pred_train_simple):.4f}\")\n",
    "print(f\"Test R^2: {r2_score(y_test, y_pred_test_simple):.4f}\")\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, y_pred_train_simple):.2f}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred_test_simple):.2f}\")\n",
    "\n",
    "# Multiple Linear Regression Evaluation\n",
    "y_pred_train_multiple = multiple_lr.predict(X_train_m)\n",
    "y_pred_test_multiple = multiple_lr.predict(X_test_m)\n",
    "\n",
    "print(\"\\nMultiple Linear Regression:\")\n",
    "print(f\"Training R^2: {r2_score(y_train_m, y_pred_train_multiple):.4f}\")\n",
    "print(f\"Test R^2: {r2_score(y_test_m, y_pred_test_multiple):.4f}\")\n",
    "print(f\"Training MSE: {mean_squared_error(y_train_m, y_pred_train_multiple):.2f}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test_m, y_pred_test_multiple):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e906c46",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "```\n",
    "Simple Linear Regression:\n",
    "Training R^2: 0.8923\n",
    "Test R^2: 0.7500\n",
    "Training MSE: 125000000.00\n",
    "Test MSE: 250000000.00\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Training R^2: 0.9500\n",
    "Test R^2: 0.9000\n",
    "Training MSE: 83333333.33\n",
    "Test MSE: 100000000.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79d609",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**:\n",
    "- **R^2**: Higher values indicate better fit. Test \\(R^2\\) lower than training \\(R^2\\) suggests possible overfitting.\n",
    "- **MSE**: Lower values indicate better accuracy. Compare training and test MSE to assess generalization.\n",
    "- Multiple linear regression often outperforms simple linear regression (higher \\(R^2\\), lower MSE) due to additional predictors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0b3da",
   "metadata": {},
   "source": [
    "## 5. Prediction and Decision Making\n",
    "\n",
    "### Prediction\n",
    "Once a model is trained, it can predict outcomes for new data. Predictions are made using the `predict` method, and the results are interpreted in the context of the problem.\n",
    "\n",
    "### Decision Making\n",
    "- **Interpret Coefficients**: In linear regression, coefficients indicate the impact of each feature. For example, a coefficient of 100 for `Size` means a 1-unit increase in size increases price by 100.\n",
    "- **Evaluate Trade-offs**: Use predictions to compare scenarios (e.g., is a larger house worth the price?).\n",
    "- **Model Selection**: Choose the model (simple, multiple, or polynomial) based on evaluation metrics, interpretability, and problem requirements.\n",
    "- **Uncertainty**: Consider prediction intervals or model limitations (e.g., extrapolation beyond training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2001981",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python Code\n",
    "# New data for prediction\n",
    "new_data = pd.DataFrame({'Size': [1900, 2500], 'Bedrooms': [3, 4]})\n",
    "\n",
    "# Predict using Multiple Linear Regression\n",
    "predictions = multiple_lr.predict(new_data)\n",
    "print(\"Predictions for new houses:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"House {i+1} (Size={new_data['Size'][i]}, Bedrooms={new_data['Bedrooms'][i]}): ${pred:.2f}\")\n",
    "\n",
    "# Decision Making Example\n",
    "# Compare two houses based on predicted price and other factors\n",
    "if predictions[0] < predictions[1]:\n",
    "    print(\"House 1 is cheaper. Consider if the extra bedroom in House 2 is worth the price difference.\")\n",
    "else:\n",
    "    print(\"House 2 is cheaper or equal. It may be a better deal with more bedrooms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f965102",
   "metadata": {},
   "source": [
    "**Sample Output**:\n",
    "```\n",
    "Predictions for new houses:\n",
    "House 1 (Size=1900, Bedrooms=3): $365000.00\n",
    "House 2 (Size=2500, Bedrooms=4): $450000.00\n",
    "House 1 is cheaper. Consider if the extra bedroom in House 2 is worth the price difference.\n",
    "\n",
    "**Explanation**:\n",
    "- **Prediction**: The model predicts prices for new houses based on `Size` and `Bedrooms`.\n",
    "- **Decision Making**: Predictions inform choices (e.g., which house to buy). Incorporate domain knowledge (e.g., budget, location) for final decisions.\n",
    "- **Coefficients**: In the multiple regression model, `Size` and `Bedrooms` coefficients help quantify their impact on price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69681f",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "- **Simple Linear Regression**: Models one predictor; easy to interpret but limited.\n",
    "- **Multiple Linear Regression**: Handles multiple predictors; more flexible but requires careful feature selection.\n",
    "- **Visualization**: Scatter plots, residual plots, and actual vs. predicted plots reveal model fit and issues.\n",
    "- **Polynomial Regression**: Captures non-linear relationships; pipelines simplify preprocessing and modeling.\n",
    "- **R-squared and MSE**: Quantify model performance; compare training and test metrics to assess generalization.\n",
    "- **Prediction and Decision Making**: Use models to predict outcomes and guide decisions, considering coefficients and context.\n",
    "\n",
    "### Key Considerations\n",
    "- **Assumptions**: Linear regression assumes linearity, independence, and normality. Check residuals to validate.\n",
    "- **Overfitting**: Polynomial regression or many predictors can overfit; use test metrics to detect.\n",
    "- **Feature Engineering**: Data preparation (e.g., normalization, indicator variables) is critical for model performance.\n",
    "- **Model Selection**: Balance complexity (simple vs. polynomial) with performance and interpretability.\n",
    "\n",
    "This code uses `scikit-learn` and `matplotlib` for modeling and visualization. Let me know if you need deeper dives into any section, alternative models, or additional examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
